# activation functions
[Site Unreachable](https://www.analyticsvidhya.com/blog/2020/01/fundamentals-deep-learning-activation-functions-when-to-use-them/)
- sigmoid: squish 1 value to (0,1) 
- softmax: get % of each values: [1.2 , 0.9 , 0.75] -> [0.42 ,Â  0.31, 0.27] (sum = 1)
- relu: value = 0 if negative -> deactivate some neurons -> more efficient  
- leaky relu: handle above relu dead neurons issue.
# Cost functions
-   Mean Squared Error
-   Mean Absolute Error
-   Hinge Loss
# Optimizer functions 
### Gradient Descent: 
> move to the steepest spot (min) of a function (usually loss func) to optimize parameters
![[Pasted image 20220924181450.png]]
- others: 
	-   Stochastic Gradient Descent
	-   Mini-Batch Gradient Descent
	-    Nesterov Accelerated Gradient
-   Momentum








# activation function
--- 
# Refererences 




2022 09 24 18:08
#cheatsheet [[machine learning]]